{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Jv7Y4hXwt0j"
   },
   "source": [
    "# [Math-Bot] Siamese LSTM:  Detecting duplicates\n",
    "\n",
    "<img src=\"media/uni_logo.png\"/>\n",
    "\n",
    "<b>Author: Alin-Andrei Georgescu 2021</b>\n",
    "\n",
    "Welcome to my notebook! It explores the Siamese networks applied to natural language processing. The model is intended to detect duplicates, in other words to check if two sentences are similar.\n",
    "The model uses \"Long short-term memory\" (LSTM) neural networks, which are an artificial recurrent neural networks (RNNs).\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [Overview](#0)\n",
    "- [Part 1: Importing the Data](#1)\n",
    "    - [1.1 Loading in the data](#1.1)\n",
    "    - [1.2 Converting a sentence to a tensor](#1.2)\n",
    "    - [1.3 Understanding and building the iterator](#1.3)\n",
    "- [Part 2: Defining the Siamese model](#2)\n",
    "    - [2.1 Understanding and building the Siamese Network](#2.1)\n",
    "    - [2.2 Implementing Hard Negative Mining](#2.2)\n",
    "- [Part 3: Training](#3)\n",
    "- [Part 4: Evaluation](#4)\n",
    "- [Part 5: Making predictions](#5)\n",
    "\n",
    "<a name='0'></a>\n",
    "### Overview\n",
    "\n",
    "General ideas:\n",
    "- Designing a Siamese networks model\n",
    "- Implementing the triplet loss\n",
    "- Evaluating accuracy\n",
    "- Using cosine similarity between the model's outputted vectors\n",
    "- Working with Trax and Numpy libraries in Python 3\n",
    "\n",
    "The LSTM cell's architecture (source: https://www.researchgate.net/figure/The-structure-of-the-LSTM-unit_fig2_331421650):\n",
    "<img src=\"https://www.researchgate.net/profile/Xiaofeng-Yuan-4/publication/331421650/figure/fig2/AS:771405641695233@1560928845927/The-structure-of-the-LSTM-unit.png\" style=\"width:600px;height:300px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "I will start by preprocessing the data, then I will build a classifier that will identify whether two sentences are the same or not. \n",
    "\n",
    "\n",
    "I tokenized the data and build a vocabulary, then split the dataset into training and testing sets. I then padded the sentences to obtain equal lengths. The model takes in the two sentence embeddings, runs them through an LSTM, and then compares the outputs of the two sub networks using cosine similarity.\n",
    "\n",
    "This notebook has been built based on Coursera's <a href=\"https://www.coursera.org/specializations/natural-language-processing\">Natural Language Processing Specialization</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sF9Hqzgwt0l"
   },
   "source": [
    "<a name='1'></a>\n",
    "# Part 1: Importing the Data\n",
    "<a name='1.1'></a>\n",
    "### 1.1 Loading in the data\n",
    "\n",
    "First step in building a model is building a dataset. I used three datasets in building my model:\n",
    "- the Quora Question Pairs\n",
    "- edited SICK dataset\n",
    "- custom Maths duplicates dataset\n",
    "\n",
    "Run the cell below to import some of the needed packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zdACgs491cs2",
    "outputId": "b31042ef-845b-46b8-c783-185e96b135f7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "\n",
    "!pip install textcleaner\n",
    "import textcleaner as tc\n",
    "\n",
    "!pip install trax\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.supervised import training\n",
    "from trax.fastmath import numpy as fastnp\n",
    "\n",
    "# set random seeds\n",
    "rnd.seed(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GYhQRMspitx"
   },
   "source": [
    "**Notice that in this notebook Trax's numpy is referred to as `fastnp`, while regular numpy is referred to as `np`.**\n",
    "\n",
    "Now the dataset will get loaded and the data processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "colab_type": "code",
    "id": "sXWBVGWnpity",
    "outputId": "afa90d4d-fed7-43b8-bcba-48c95d600ad5"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/merged_dataset.csv\", encoding=\"utf-8\")\n",
    "\n",
    "N = len(data)\n",
    "print(\"Number of sentence pairs: \", N)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkSQTu7Ypit0"
   },
   "source": [
    "Then I split the data into a train and test set. The test set will be used later to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z00A7vEMpit1",
    "outputId": "c12ae7e8-a959-4f56-aa29-6ad34abc1c81"
   },
   "outputs": [],
   "source": [
    "N_dups = len(data[data.is_duplicate == 1])\n",
    "\n",
    "# Take 90% of the duplicates for the train set\n",
    "N_train = int(N_dups * 0.9)\n",
    "print(N_train)\n",
    "\n",
    "# Take the rest of the duplicates for the test set + an equal number of non-dups\n",
    "N_test = (N_dups - N_train) * 2\n",
    "print(N_test)\n",
    "\n",
    "data_train = data[: N_train]\n",
    "# Shuffle the train set\n",
    "data_train = data_train.sample(frac=1)\n",
    "\n",
    "data_test = data[N_train : N_train + N_test]\n",
    "# Shuffle the test set\n",
    "data_test = data_test.sample(frac=1)\n",
    "\n",
    "print(\"Train set: \", len(data_train), \"; Test set: \", len(data_test))\n",
    "\n",
    "# Remove the unneeded data to some memory\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHpZO58Dss_v"
   },
   "outputs": [],
   "source": [
    "S1_train_words = np.array(data_train[\"sentence1\"])\n",
    "S2_train_words = np.array(data_train[\"sentence2\"])\n",
    "\n",
    "S1_test_words = np.array(data_test[\"sentence1\"])\n",
    "S2_test_words = np.array(data_test[\"sentence2\"])\n",
    "y_test  = np.array(data_test[\"is_duplicate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5vBkxunpiuB"
   },
   "source": [
    "Above, you have seen that the model only takes the duplicated sentences for training.\n",
    "All this has a purpose, as the data generator will produce batches $([s1_1, s1_2, s1_3, ...]$, $[s2_1, s2_2,s2_3, ...])$, where $s1_i$ and $s2_k$ are duplicate if and only if $i = k$.\n",
    "\n",
    "An example of how the data looks is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "joyrS1XEpLWn",
    "outputId": "3257cde7-3164-40d9-910e-fa91eae917a0"
   },
   "outputs": [],
   "source": [
    "print(\"TRAINING SENTENCES:\\n\")\n",
    "print(\"Sentence 1: \", S1_train_words[0])\n",
    "print(\"Sentence 2: \", S2_train_words[0], \"\\n\")\n",
    "print(\"Sentence 1: \", S1_train_words[5])\n",
    "print(\"Sentence 2: \", S2_train_words[5], \"\\n\")\n",
    "\n",
    "print(\"TESTING SENTENCES:\\n\")\n",
    "print(\"Sentence 1: \", S1_test_words[0])\n",
    "print(\"Sentence 2: \", S2_test_words[0], \"\\n\")\n",
    "print(\"is_duplicate =\", y_test[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WC_BZU3XpiuF"
   },
   "source": [
    "The first step is to tokenize the sentences using a custom tokenizer defined below. Then each word of the selected duplicate pairs is encoded with an index. After that, given a sentence, it can just be encoded as a list of numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbCoIgLQpiuF"
   },
   "outputs": [],
   "source": [
    "# Create arrays\n",
    "S1_train = np.empty_like(S1_train_words)\n",
    "S2_train = np.empty_like(S2_train_words)\n",
    "\n",
    "S1_test = np.empty_like(S1_test_words)\n",
    "S2_test = np.empty_like(S2_test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tokenizer(sentence):\n",
    "    \"\"\"Tokenizer function - cleans and tokenizes the data\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence.\n",
    "    Returns:\n",
    "        list: The transformed input sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    if sentence == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    sentence = tc.lower_all(sentence)[0]\n",
    "\n",
    "    # Change tabs to spaces\n",
    "    sentence = re.sub(r\"\\t+_+\", \" \", sentence)\n",
    "    # Change short forms\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"(can\\'t|can not)\", \"cannot\", sentence)\n",
    "    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"I\\'m\", \"I am\", sentence)\n",
    "    sentence = re.sub(r\" m \", \" am \", sentence)\n",
    "    sentence = re.sub(r\"(\\'re| r )\", \" are \", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would \", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will \", sentence)\n",
    "    sentence = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", sentence)\n",
    "    # Make word separations\n",
    "    sentence = re.sub(r\"(\\+|-|\\*|\\/|\\^|\\.)\", \" $1 \", sentence)\n",
    "    # Remove irrelevant stuff, nonprintable characters and spaces\n",
    "    sentence = re.sub(r\"(\\'s|\\'S|\\'|\\\"|,|[^ -~]+)\", \"\", sentence)\n",
    "    sentence = tc.strip_all(sentence)[0]\n",
    "    # Remove dot (encoded by textcleaner with $1), if necessary\n",
    "    sentence = re.sub(r\" *\\$1 *$\", \"\", sentence)\n",
    "\n",
    "    if sentence == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    return tc.token_it(tc.lemming(tc.stemming(sentence)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "m9ZmfpGWpiuI",
    "outputId": "d2995c9a-92b4-4892-d34b-c77b94b27134"
   },
   "outputs": [],
   "source": [
    "# Building the vocabulary with the train set\n",
    "from collections import defaultdict\n",
    "\n",
    "vocab = defaultdict(lambda: 0)\n",
    "vocab[\"<PAD>\"] = 1\n",
    "\n",
    "for idx in range(len(S1_train_words)):\n",
    "    S1_train[idx] = data_tokenizer(S1_train_words[idx])\n",
    "    S2_train[idx] = data_tokenizer(S2_train_words[idx])\n",
    "\n",
    "    s = S1_train[idx] + S2_train[idx]\n",
    "    for word in s:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1\n",
    "\n",
    "print(\"The length of the vocabulary is: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "TTMRF8eZpiuK",
    "outputId": "f81d4dc1-7cf9-4476-a454-467b54fe4dc4"
   },
   "outputs": [],
   "source": [
    "print(vocab[\"<PAD>\"])\n",
    "print(vocab[\"Astrology\".lower()])\n",
    "print(vocab[\"Scrumptious\".lower()])  #not in vocabulary, returns 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5sDs36m81g6f"
   },
   "outputs": [],
   "source": [
    "for idx in range(len(S1_test_words)): \n",
    "    S1_test[idx] = data_tokenizer(S1_test_words[idx])\n",
    "    S2_test[idx] = data_tokenizer(S2_test_words[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BDcxEmX31y3d"
   },
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 Converting a sentence to a tensor\n",
    "\n",
    "The next step is to convert every sentence to a tensor, or an array of numbers, using the vocabulary built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOhNa-sapiuS"
   },
   "outputs": [],
   "source": [
    "# Converting sentences to arrays of integers\n",
    "for i in range(len(S1_train)):\n",
    "    S1_train[i] = [vocab[word] for word in S1_train[i]]\n",
    "    S2_train[i] = [vocab[word] for word in S2_train[i]]\n",
    "\n",
    "        \n",
    "for i in range(len(S1_test)):\n",
    "    S1_test[i] = [vocab[word] for word in S1_test[i]]\n",
    "    S2_test[i] = [vocab[word] for word in S2_test[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Dpawm38dpiuU",
    "outputId": "ef1aa65b-c89b-46f9-a9cf-f73748f1ee56"
   },
   "outputs": [],
   "source": [
    "print(\"FIRST SENTENCE IN TRAIN SET:\\n\")\n",
    "print(S1_train_words[0], \"\\n\") \n",
    "print(\"ENCODED VERSION:\")\n",
    "print(S1_train[0],\"\\n\")\n",
    "\n",
    "print(\"FIRST SENTENCE IN TEST SET:\\n\")\n",
    "print(S1_test_words[0], \"\\n\")\n",
    "print(\"ENCODED VERSION:\")\n",
    "print(S1_test[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SuggGPaQpiuY"
   },
   "source": [
    "Now, the train set must be split into a training/validation set so that it can be used to train and evaluate the Siamese model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "BmhrWPtgpiuY",
    "outputId": "7272fb74-79e6-499a-ce95-d11b9edcd64a"
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "cut_off = int(len(S1_train) * .8)\n",
    "train_S1, train_S2 = S1_train[: cut_off], S2_train[: cut_off]\n",
    "val_S1, val_S2 = S1_train[cut_off :], S2_train[cut_off :]\n",
    "print(\"Number of duplicate sentences: \", len(S1_train))\n",
    "print(\"The length of the training set is:  \", len(train_S1))\n",
    "print(\"The length of the validation set is: \", len(val_S1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFOR19cX2TQs"
   },
   "source": [
    "<a name='1.3'></a>\n",
    "### 1.3 Understanding and building the iterator \n",
    "\n",
    "Given the compational limits, we need to split our data into batches. In this notebook, I built a data generator that takes in $S1$ and $S2$ and returned a batch of size `batch_size` in the following format $([s1_1, s1_2, s1_3, ...]$, $[s2_1, s2_2,s2_3, ...])$. The tuple consists of two arrays and each array has `batch_size` sentences. Again, $s1_i$ and $s2_i$ are duplicates, but they are not duplicates with any other elements in the batch. \n",
    "\n",
    "The command `next(data_generator)` returns the next batch. This iterator returns a pair of arrays of sentences, which will later be used in the model.\n",
    "\n",
    "**The ideas behind:**  \n",
    "- The generator returns shuffled batches of data. To achieve this without modifying the actual sentence lists, a list containing the indexes of the sentences is created. This list can be shuffled and used to get random batches everytime the index is reset.\n",
    "- Append elements of $S1$ and $S2$ to `input1` and `input2` respectively.\n",
    "- If batches are full (i.e. we have achieved `batch_size` length), determine `max_len` as the longest sentence in `input1` and `input2`. I ceiled `max_len` to a power of $2$, for computation purposes.\n",
    "- The shorter sentences are padded by `vocab[\"<PAD>\"]` until the length `max_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibchgos48MtA"
   },
   "outputs": [],
   "source": [
    "def data_generator(S1, S2, batch_size, pad=1, shuffle=False):\n",
    "    \"\"\"Generator function that yields batches of data\n",
    "\n",
    "    Args:\n",
    "        S1 (list): List of transformed (to tensor) sentences.\n",
    "        S2 (list): List of transformed (to tensor) sentences.\n",
    "        batch_size (int): Number of elements per batch.\n",
    "        pad (int, optional): Pad character from the vocab. Defaults to 1.\n",
    "        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to False.\n",
    "    Yields:\n",
    "        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)\n",
    "        NOTE: input1: inputs to your model [s1a, s2a, s3a, ...] i.e. (s1a,s1b) are duplicates\n",
    "              input2: targets to your model [s1b, s2b,s3b, ...] i.e. (s1a,s2i) i!=a are not duplicates\n",
    "    \"\"\"\n",
    "\n",
    "    input1 = []\n",
    "    input2 = []\n",
    "    idx = 0\n",
    "    len_s = len(S1)\n",
    "    sentence_indexes = [*range(len_s)]\n",
    "\n",
    "    if shuffle:\n",
    "        rnd.shuffle(sentence_indexes)\n",
    "\n",
    "    while True:\n",
    "        if idx >= len_s:\n",
    "            # If idx is greater than or equal to len_q, reset it\n",
    "            idx = 0\n",
    "            # Shuffle to get random batches if shuffle is set to True\n",
    "            if shuffle:\n",
    "                rnd.shuffle(sentence_indexes)\n",
    "\n",
    "        s1 = S1[sentence_indexes[idx]]\n",
    "        s2 = S2[sentence_indexes[idx]]\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "        input1.append(s1)\n",
    "        input2.append(s2)\n",
    "\n",
    "        if len(input1) == batch_size:\n",
    "            # Determine max_len as the longest sentence in input1 & input 2\n",
    "            max_len = max(max([len(s) for s in input1]), max([len(s) for s in input2]))\n",
    "            # Pad to power-of-2\n",
    "            max_len = 2 ** int(np.ceil(np.log2(max_len)))\n",
    "\n",
    "            b1 = []\n",
    "            b2 = []\n",
    "            for s1, s2 in zip(input1, input2):\n",
    "                # Add [pad] to s1 until it reaches max_len\n",
    "                s1 = s1 + [pad] * (max_len - len(s1))\n",
    "                # Add [pad] to s2 until it reaches max_len\n",
    "                s2 = s2 + [pad] * (max_len - len(s2))\n",
    "\n",
    "                # Append s1\n",
    "                b1.append(s1)\n",
    "                # Append s2\n",
    "                b2.append(s2)\n",
    "\n",
    "            # Use b1 and b2\n",
    "            yield np.array(b1), np.array(b2)\n",
    "\n",
    "            # reset the batches\n",
    "            input1, input2 = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ZFZeBPnW8Mlb",
    "outputId": "7a31cd19-55dc-4b97-f288-6c59c6a34b53",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "res1, res2 = next(data_generator(train_S1, train_S2, batch_size))\n",
    "print(\"First sentences  :\\n\", res1, \"\\n\")\n",
    "print(\"Second sentences :\\n\", res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tWJ1L9m2piui"
   },
   "source": [
    "Now we can go ahead and start building the neural network, as we have a data generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KmZRBoaMwt0w"
   },
   "source": [
    "<a name='2'></a>\n",
    "# Part 2: Defining the Siamese model\n",
    "\n",
    "<a name='2.1'></a>\n",
    "\n",
    "### 2.1 Understanding and building the Siamese Network \n",
    "\n",
    "A Siamese network is a neural network which uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. The Siamese network model proposed in this notebook looks like this:\n",
    "\n",
    "<img src=\"media/siamese.png\" style=\"width:600px;height:300px;\"/>\n",
    "\n",
    "The sentences' embeddings are passed to an LSTM layer, the output vectors, $v_1$ and $v_2$, are normalized, and finally a triplet loss is used to get the corresponding cosine similarity for each pair of sentences. The triplet loss makes use of a baseline (anchor) input that is compared to a positive (truthy) input and a negative (falsy) input. The distance from the baseline (anchor) input to the positive (truthy) input is minimized, and the distance from the baseline (anchor) input to the negative (falsy) input is maximized. In math equations, the following is maximized:\n",
    "\n",
    "$$\\mathcal{L}(A, P, N)=\\max \\left(\\|\\mathrm{f}(A)-\\mathrm{f}(P)\\|^{2}-\\|\\mathrm{f}(A)-\\mathrm{f}(N)\\|^{2}+\\alpha, 0\\right)$$\n",
    "\n",
    "$A$ is the anchor input, for example $s1_1$, $P$ the duplicate input, for example, $s2_1$, and $N$ the negative input (the non duplicate sentence), for example $s2_2$.<br>\n",
    "$\\alpha$ is a margin - how much the duplicates are pushed from the non duplicates. \n",
    "<br>\n",
    "\n",
    "**The ideas behind:**\n",
    "- Trax library is used in implementing the model.\n",
    "- `tl.Serial`: Combinator that applies layers serially (by function composition) allowing the set up the overall structure of the feedforward.\n",
    "- `tl.Embedding`: Maps discrete tokens to vectors. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (also called d_feature) is the number of elements in the word embedding.\n",
    "- `tl.LSTM` The LSTM layer.    \n",
    "- `tl.Mean`: Computes the mean across a desired axis. Mean uses one tensor axis to form groups of values and replaces each group with the mean value of that group.\n",
    "- `tl.Fn` Layer with no weights that applies the function f - vector normalization in this case.\n",
    "- `tl.parallel`: It is a combinator layer (like `Serial`) that applies a list of layers in parallel to its inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hww76f8_wt0x"
   },
   "outputs": [],
   "source": [
    "def Siamese(vocab_size=len(vocab), d_model=128, mode=\"train\"):\n",
    "    \"\"\"Returns a Siamese model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int, optional): Length of the vocabulary. Defaults to len(vocab).\n",
    "        d_model (int, optional): Depth of the model. Defaults to 128.\n",
    "        mode (str, optional): \"train\", \"eval\" or \"predict\", predict mode is for fast inference. Defaults to \"train\".\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Parallel: A Siamese model. \n",
    "    \"\"\"\n",
    "\n",
    "    def normalize(x):  # normalizes the vectors to have L2 norm 1\n",
    "        return x / fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n",
    "\n",
    "    s_processor = tl.Serial(                        # Processor will run on S1 and S2.\n",
    "        tl.Embedding(vocab_size, d_model),          # Embedding layer\n",
    "        tl.LSTM(d_model),                           # LSTM layer\n",
    "        tl.Mean(axis=1),                            # Mean over columns\n",
    "        tl.Fn('Normalize', lambda x: normalize(x))  # Apply normalize function\n",
    "    )  # Returns one vector of shape [batch_size, d_model].\n",
    "    \n",
    "    # Run on S1 and S2 in parallel.\n",
    "    model = tl.Parallel(s_processor, s_processor)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "es2gfwZypiul"
   },
   "source": [
    "Setup the Siamese network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "kvQ_jf52-JAn",
    "outputId": "d409460d-2ffb-4ae6-8745-ddcfa1d892ad"
   },
   "outputs": [],
   "source": [
    "# Check the model\n",
    "model = Siamese(d_model=256)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVo1Gvripiuo"
   },
   "source": [
    "<a name='2.2'></a>\n",
    "\n",
    "### 2.2 Implementing Hard  Negative Mining\n",
    "\n",
    "\n",
    "Now it's the time to implement the `TripletLoss`.\n",
    "As explained earlier, loss is composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the *closest negative*. The loss expression is then:\n",
    " \n",
    "\\begin{align}\n",
    " \\mathcal{Loss_1(A,P,N)} &=\\max \\left( -cos(A,P)  + mean_{neg} +\\alpha, 0\\right) \\\\\n",
    " \\mathcal{Loss_2(A,P,N)} &=\\max \\left( -cos(A,P)  + closest_{neg} +\\alpha, 0\\right) \\\\\n",
    "\\mathcal{Loss(A,P,N)} &= mean(Loss_1 + Loss_2) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJM8EQiopiuv"
   },
   "outputs": [],
   "source": [
    "def TripletLossFn(v1, v2, margin=0.25):\n",
    "    \"\"\"Custom Loss function.\n",
    "\n",
    "    Args:\n",
    "        v1 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to S1.\n",
    "        v2 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to S2.\n",
    "        margin (float, optional): Desired margin. Defaults to 0.25.\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Triplet Loss.\n",
    "    \"\"\"    \n",
    "    scores = fastnp.dot(v1, v2.T)       # pairwise cosine sim\n",
    "    batch_size = len(scores)\n",
    "\n",
    "    positive = fastnp.diagonal(scores)  # the positive ones (duplicates)\n",
    "    negative_without_positive = scores - 2.0 * fastnp.eye(batch_size)\n",
    "\n",
    "    closest_negative = fastnp.max(negative_without_positive, axis=1)\n",
    "    negative_zero_on_duplicate = (1.0 - fastnp.eye(batch_size)) * scores\n",
    "    mean_negative = fastnp.sum(negative_zero_on_duplicate, axis=1) / (batch_size - 1)\n",
    "\n",
    "    triplet_loss1 = fastnp.maximum(0.0, margin - positive + closest_negative)\n",
    "    triplet_loss2 = fastnp.maximum(0.0, margin - positive + mean_negative)\n",
    "    triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n",
    "    \n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
    "v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
    "TripletLossFn(v2,v1)\n",
    "print(\"Triplet Loss:\", TripletLossFn(v2,v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "Triplet Loss: 0.5\n",
    "```   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def TripletLoss(margin=0.25):\n",
    "    # Trax layer creation\n",
    "    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n",
    "    return tl.Fn(\"TripletLoss\", triplet_loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsvjaCQ6wt02"
   },
   "source": [
    "<a name='3'></a>\n",
    "\n",
    "# Part 3: Training\n",
    "\n",
    "The next step is model training - defining the cost function and the optimizer, feeding in the built model. But first I will define the data generators used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iPk7gh-nzCBg",
    "outputId": "a2e8525d-f89a-4d9d-c0d6-bd7406f0246a"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_generator = data_generator(train_S1, train_S2, batch_size, vocab[\"<PAD>\"])\n",
    "val_generator = data_generator(val_S1, val_S2, batch_size, vocab[\"<PAD>\"])\n",
    "print(\"train_S1.shape \", train_S1.shape)\n",
    "print(\"val_S1.shape   \", val_S1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IgFMfH5awt07"
   },
   "source": [
    "Now, I will define the training step. Each training iteration is defined as an `epoch`, each epoch being an iteration over all the data, using the training iterator.\n",
    "\n",
    "**The ideas behind:**\n",
    "- Two tasks are needed: `TrainTask` and `EvalTask`.\n",
    "- The training runs in a trax loop `trax.supervised.training.Loop`.\n",
    "- Pass the other parameters to a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kbtfz4T_m7x"
   },
   "outputs": [],
   "source": [
    "def train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir=\"trax_model/\"):\n",
    "    \"\"\"Training the Siamese Model\n",
    "\n",
    "    Args:\n",
    "        Siamese (function): Function that returns the Siamese model.\n",
    "        TripletLoss (function): Function that defines the TripletLoss loss function.\n",
    "        lr_schedule (function): Trax multifactor schedule function.\n",
    "        train_generator (generator, optional): Training generator. Defaults to train_generator.\n",
    "        val_generator (generator, optional): Validation generator. Defaults to val_generator.\n",
    "        output_dir (str, optional): Path to save model to. Defaults to \"trax_model/\".\n",
    "\n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop for the model.\n",
    "    \"\"\"\n",
    "    output_dir = os.path.expanduser(output_dir)\n",
    "\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=train_generator,\n",
    "        loss_layer=TripletLoss(),\n",
    "        optimizer=trax.optimizers.Adam(0.01),\n",
    "        lr_schedule=lr_schedule\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=val_generator,\n",
    "        metrics=[TripletLoss()]\n",
    "    )\n",
    "\n",
    "    training_loop = training.Loop(Siamese(),\n",
    "                                  train_task,\n",
    "                                  eval_tasks=[eval_task],\n",
    "                                  output_dir=output_dir,\n",
    "                                  random_seed=34)\n",
    "\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "-3KXjmBo_6Xa",
    "outputId": "9d57f731-1534-4218-e744-783359d5cd19"
   },
   "outputs": [],
   "source": [
    "train_steps = 1500\n",
    "lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\n",
    "training_loop = train_model(Siamese, TripletLoss, lr_schedule)\n",
    "training_loop.run(train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abKPe7d4wt1C"
   },
   "source": [
    "<a name='4'></a>\n",
    "\n",
    "# Part 4:  Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDi4MBiKpivF"
   },
   "source": [
    "To determine the accuracy of the model, the test set that was configured earlier is used. While the training used only positive examples, the test data, S1_test, S2_test and y_test, is setup as pairs of sentences, some of which are duplicates some are not. \n",
    "This routine runs all the test sentences pairs through the model, computes the cosine simlarity of each pair, thresholds it and compares the result to  y_test - the correct response from the data set. The results are accumulated to produce an accuracy.\n",
    "\n",
    "**The ideas behind:**  \n",
    " - The model loops through the incoming data in batch_size chunks.\n",
    " - The output vectors are computed and their cosine similarity is thresholded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-h6ZH507fUm"
   },
   "outputs": [],
   "source": [
    "def classify(test_S1, test_S2, y, threshold, model, vocab, data_generator=data_generator, batch_size=64):\n",
    "    \"\"\"Function to test the accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "        test_S1 (numpy.ndarray): Array of S1 sentences.\n",
    "        test_S2 (numpy.ndarray): Array of S2 sentences.\n",
    "        y (numpy.ndarray): Array of actual target.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (trax.layers.combinators.Parallel): The Siamese model.\n",
    "        vocab (collections.defaultdict): The vocabulary used.\n",
    "        data_generator (function): Data generator function. Defaults to data_generator.\n",
    "        batch_size (int, optional): Size of the batches. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model.\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "\n",
    "    for i in range(0, len(test_S1), batch_size):\n",
    "        to_process = len(test_S1) - i\n",
    "\n",
    "        if to_process < batch_size:\n",
    "            batch_size = to_process\n",
    "\n",
    "        s1, s2 = next(data_generator(test_S1[i : i + batch_size], test_S2[i : i + batch_size], batch_size, vocab[\"<PAD>\"], shuffle=False))\n",
    "        y_test = y[i : i + batch_size]\n",
    "\n",
    "        v1, v2 = model((s1, s2))\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            d = np.dot(v1[j], v2[j].T)\n",
    "            res = d > threshold\n",
    "\n",
    "            accuracy += (y_test[j] == res)\n",
    "\n",
    "    accuracy = accuracy / len(test_S1)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(S1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yeQjHxkfpivH",
    "outputId": "103b8449-896f-403d-f011-583df70afdae"
   },
   "outputs": [],
   "source": [
    "# Loading in the saved model\n",
    "model = Siamese()\n",
    "model.init_from_file(\"trax_model/model.pkl.gz\")\n",
    "# Evaluating it\n",
    "accuracy = classify(S1_test, S2_test, y_test, 0.7, model, vocab, batch_size=512) \n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21h3Y0FNpivK"
   },
   "source": [
    "<a name='5'></a>\n",
    "\n",
    "# Part 5: Making predictions\n",
    "\n",
    "In this section the model will be put to work. It will be wrapped in a function called `predict` which takes two sentences as input and returns $1$ or $0$, depending on whether the pair is a duplicate or not.   \n",
    "\n",
    "But first, a reverse vocabulary needs to be built, because it allows to map encoded sentences back to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kg0wQ8qhpivL"
   },
   "outputs": [],
   "source": [
    "def predict(sentence1, sentence2, threshold, model, vocab, data_generator=data_generator, verbose=False):\n",
    "    \"\"\"Function for predicting if two sentences are duplicates.\n",
    "\n",
    "    Args:\n",
    "        sentence1 (str): First sentence.\n",
    "        sentence2 (str): Second sentence.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (trax.layers.combinators.Parallel): The Siamese model.\n",
    "        vocab (collections.defaultdict): The vocabulary used.\n",
    "        data_generator (function): Data generator function. Defaults to data_generator.\n",
    "        verbose (bool, optional): If the results should be printed out. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the sentences are duplicates, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    s1 = data_tokenizer(sentence1)  # tokenize\n",
    "    s2 = data_tokenizer(sentence2)  # tokenize\n",
    "    S1, S2 = [], []\n",
    "\n",
    "    for word in s1:  # encode s1\n",
    "        S1 += [vocab[word]]\n",
    "    for word in s2:  # encode s2\n",
    "        S2 += [vocab[word]]\n",
    "\n",
    "    S1, S2 = next(data_generator([S1], [S2], 1, vocab[\"<PAD>\"]))\n",
    "\n",
    "    v1, v2 = model((S1, S2))\n",
    "    d = np.dot(v1[0], v2[0].T)\n",
    "    res = d > threshold\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(\"S1  = \", S1, \"\\nS2  = \", S2)\n",
    "        print(\"d   = \", d)\n",
    "        print(\"res = \", res)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the model's ability to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Raojyhw3z7HE",
    "outputId": "b0907aaf-63c0-448d-99b0-012359381a97"
   },
   "outputs": [],
   "source": [
    "sentence1 = \"I love running in the park.\"\n",
    "sentence2 = \"I like running in park.\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(sentence1 , sentence2, 0.7, model, vocab, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NAfV3l5Zwt1L"
   },
   "source": [
    "The Siamese network is capable of catching complicated structures. Concretely, it can identify sentence duplicates although the sentences do not have many words in common."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "C3_W4_Assignment_Solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "coursera": {
   "schema_names": [
    "NLPC3-4A"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}