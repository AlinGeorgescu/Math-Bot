{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Jv7Y4hXwt0j"
   },
   "source": [
    "# [Math-Bot] Siamese LSTM:  Detecting duplicates\n",
    "\n",
    "<img src=\"media/uni_logo.png\"/>\n",
    "\n",
    "<b>Author: Alin-Andrei Georgescu 2021</b>\n",
    "\n",
    "Welcome to my notebook! It explores the Siamese networks applied to natural language processing. The model is intended to detect duplicates, in other words to check if two sentences are similar.\n",
    "The model uses \"Long short-term memory\" (LSTM) neural networks, which are an artificial recurrent neural networks (RNNs). This version uses GloVe pretrained vectors.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [Overview](#0)\n",
    "- [Part 1: Importing the Data](#1)\n",
    "    - [1.1 Loading in the data](#1.1)\n",
    "    - [1.2 Converting a sentence to a tensor](#1.2)\n",
    "    - [1.3 Understanding and building the iterator](#1.3)\n",
    "- [Part 2: Defining the Siamese model](#2)\n",
    "    - [2.1 Understanding and building the Siamese Network](#2.1)\n",
    "    - [2.2 Implementing Hard Negative Mining](#2.2)\n",
    "- [Part 3: Training](#3)\n",
    "- [Part 4: Evaluation](#4)\n",
    "- [Part 5: Making predictions](#5)\n",
    "\n",
    "<a name='0'></a>\n",
    "### Overview\n",
    "\n",
    "General ideas:\n",
    "- Designing a Siamese networks model\n",
    "- Implementing the triplet loss\n",
    "- Evaluating accuracy\n",
    "- Using cosine similarity between the model's outputted vectors\n",
    "- Working with Trax and Numpy libraries in Python 3\n",
    "\n",
    "The LSTM cell's architecture (source: https://www.researchgate.net/figure/The-structure-of-the-LSTM-unit_fig2_331421650):\n",
    "<img src=\"https://www.researchgate.net/profile/Xiaofeng-Yuan-4/publication/331421650/figure/fig2/AS:771405641695233@1560928845927/The-structure-of-the-LSTM-unit.png\" style=\"width:600px;height:300px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "I will start by preprocessing the data, then I will build a classifier that will identify whether two sentences are the same or not. \n",
    "\n",
    "\n",
    "I tokenized the data, then split the dataset into training and testing sets. I loaded pretrained GloVe word embeddings and built a sentence's vector by averaging the composing word's vectors. The model takes in the two sentence embeddings, runs them through an LSTM, and then compares the outputs of the two sub networks using cosine similarity.\n",
    "\n",
    "This notebook has been built based on Coursera's <a href=\"https://www.coursera.org/specializations/natural-language-processing\">Natural Language Processing Specialization</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sF9Hqzgwt0l"
   },
   "source": [
    "<a name='1'></a>\n",
    "# Part 1: Importing the Data\n",
    "<a name='1.1'></a>\n",
    "### 1.1 Loading in the data\n",
    "\n",
    "First step in building a model is building a dataset. I used three datasets in building my model:\n",
    "- the Quora Question Pairs\n",
    "- edited SICK dataset\n",
    "- custom Maths duplicates dataset\n",
    "\n",
    "Run the cell below to import some of the needed packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T15:43:29.504370Z",
     "iopub.status.busy": "2021-06-07T15:43:29.503900Z",
     "iopub.status.idle": "2021-06-07T15:47:00.816742Z",
     "shell.execute_reply": "2021-06-07T15:47:00.815196Z",
     "shell.execute_reply.started": "2021-06-07T15:43:29.504278Z"
    },
    "id": "zdACgs491cs2",
    "outputId": "b31042ef-845b-46b8-c783-185e96b135f7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "\n",
    "!pip install textcleaner\n",
    "import textcleaner as tc\n",
    "\n",
    "!pip install trax\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.supervised import training\n",
    "from trax.fastmath import numpy as fastnp\n",
    "\n",
    "!pip install gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# set random seeds\n",
    "rnd.seed(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GYhQRMspitx"
   },
   "source": [
    "**Notice that in this notebook Trax's numpy is referred to as `fastnp`, while regular numpy is referred to as `np`.**\n",
    "\n",
    "Now the dataset and word embeddings will get loaded and the data processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:28:23.141177Z",
     "iopub.status.busy": "2021-06-07T17:28:23.140801Z",
     "iopub.status.idle": "2021-06-07T17:28:25.426722Z",
     "shell.execute_reply": "2021-06-07T17:28:25.425572Z",
     "shell.execute_reply.started": "2021-06-07T17:28:23.141146Z"
    },
    "id": "sXWBVGWnpity",
    "outputId": "afa90d4d-fed7-43b8-bcba-48c95d600ad5"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/merged_dataset.csv\", encoding=\"utf-8\")\n",
    "\n",
    "N = len(data)\n",
    "print(\"Number of sentence pairs: \", N)\n",
    "\n",
    "data.head()\n",
    "\n",
    "!wget -O data/glove.840B.300d.zip nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!unzip -d data data/glove.840B.300d.zip\n",
    "!rm data/glove.840B.300d.zip\n",
    "vec_model = KeyedVectors.load_word2vec_format(\"data/glove.840B.300d.txt\", binary=False, no_header=True)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkSQTu7Ypit0"
   },
   "source": [
    "Then I split the data into a train and test set. The test set will be used later to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:28:29.933443Z",
     "iopub.status.busy": "2021-06-07T17:28:29.933019Z",
     "iopub.status.idle": "2021-06-07T17:28:30.010395Z",
     "shell.execute_reply": "2021-06-07T17:28:30.009345Z",
     "shell.execute_reply.started": "2021-06-07T17:28:29.933406Z"
    },
    "id": "z00A7vEMpit1",
    "outputId": "c12ae7e8-a959-4f56-aa29-6ad34abc1c81"
   },
   "outputs": [],
   "source": [
    "N_dups = len(data[data.is_duplicate == 1])\n",
    "\n",
    "# Take 90% of the duplicates for the train set\n",
    "N_train = int(N_dups * 0.9)\n",
    "print(N_train)\n",
    "\n",
    "# Take the rest of the duplicates for the test set + an equal number of non-dups\n",
    "N_test = (N_dups - N_train) * 2\n",
    "print(N_test)\n",
    "\n",
    "data_train = data[: N_train]\n",
    "# Shuffle the train set\n",
    "data_train = data_train.sample(frac=1)\n",
    "\n",
    "data_test = data[N_train : N_train + N_test]\n",
    "# Shuffle the test set\n",
    "data_test = data_test.sample(frac=1)\n",
    "\n",
    "print(\"Train set: \", len(data_train), \"; Test set: \", len(data_test))\n",
    "\n",
    "# Remove the unneeded data to some memory\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:28:32.490507Z",
     "iopub.status.busy": "2021-06-07T17:28:32.490145Z",
     "iopub.status.idle": "2021-06-07T17:28:32.511368Z",
     "shell.execute_reply": "2021-06-07T17:28:32.510559Z",
     "shell.execute_reply.started": "2021-06-07T17:28:32.490478Z"
    },
    "id": "XHpZO58Dss_v"
   },
   "outputs": [],
   "source": [
    "S1_train_words = np.array(data_train[\"sentence1\"])\n",
    "S2_train_words = np.array(data_train[\"sentence2\"])\n",
    "\n",
    "S1_test_words = np.array(data_test[\"sentence1\"])\n",
    "S2_test_words = np.array(data_test[\"sentence2\"])\n",
    "y_test  = np.array(data_test[\"is_duplicate\"])\n",
    "\n",
    "del(data_train)\n",
    "del(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5vBkxunpiuB"
   },
   "source": [
    "Above, you have seen that the model only takes the duplicated sentences for training.\n",
    "All this has a purpose, as the data generator will produce batches $([s1_1, s1_2, s1_3, ...]$, $[s2_1, s2_2,s2_3, ...])$, where $s1_i$ and $s2_k$ are duplicate if and only if $i = k$.\n",
    "\n",
    "An example of how the data looks is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:28:36.235570Z",
     "iopub.status.busy": "2021-06-07T17:28:36.234457Z",
     "iopub.status.idle": "2021-06-07T17:28:36.245286Z",
     "shell.execute_reply": "2021-06-07T17:28:36.244268Z",
     "shell.execute_reply.started": "2021-06-07T17:28:36.235526Z"
    },
    "id": "joyrS1XEpLWn",
    "outputId": "3257cde7-3164-40d9-910e-fa91eae917a0"
   },
   "outputs": [],
   "source": [
    "print(\"TRAINING SENTENCES:\\n\")\n",
    "print(\"Sentence 1: \", S1_train_words[0])\n",
    "print(\"Sentence 2: \", S2_train_words[0], \"\\n\")\n",
    "print(\"Sentence 1: \", S1_train_words[5])\n",
    "print(\"Sentence 2: \", S2_train_words[5], \"\\n\")\n",
    "\n",
    "print(\"TESTING SENTENCES:\\n\")\n",
    "print(\"Sentence 1: \", S1_test_words[0])\n",
    "print(\"Sentence 2: \", S2_test_words[0], \"\\n\")\n",
    "print(\"is_duplicate =\", y_test[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WC_BZU3XpiuF"
   },
   "source": [
    "The first step is to tokenize the sentences using a custom tokenizer defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:28:38.408977Z",
     "iopub.status.busy": "2021-06-07T17:28:38.408608Z",
     "iopub.status.idle": "2021-06-07T17:28:38.414623Z",
     "shell.execute_reply": "2021-06-07T17:28:38.413898Z",
     "shell.execute_reply.started": "2021-06-07T17:28:38.408946Z"
    },
    "id": "QbCoIgLQpiuF"
   },
   "outputs": [],
   "source": [
    "# Create arrays\n",
    "S1_train = np.empty_like(S1_train_words)\n",
    "S2_train = np.empty_like(S2_train_words)\n",
    "\n",
    "S1_test = np.empty_like(S1_test_words)\n",
    "S2_test = np.empty_like(S2_test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:28:39.669553Z",
     "iopub.status.busy": "2021-06-07T17:28:39.668668Z",
     "iopub.status.idle": "2021-06-07T17:28:39.678441Z",
     "shell.execute_reply": "2021-06-07T17:28:39.677646Z",
     "shell.execute_reply.started": "2021-06-07T17:28:39.669515Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_tokenizer(sentence):\n",
    "    \"\"\"Tokenizer function - cleans and tokenizes the data\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence.\n",
    "    Returns:\n",
    "        list: The transformed input sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    if sentence == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    sentence = tc.lower_all(sentence)[0]\n",
    "\n",
    "    # Change tabs to spaces\n",
    "    sentence = re.sub(r\"\\t+_+\", \" \", sentence)\n",
    "    # Change short forms\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"(can\\'t|can not)\", \"cannot\", sentence)\n",
    "    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"I\\'m\", \"I am\", sentence)\n",
    "    sentence = re.sub(r\" m \", \" am \", sentence)\n",
    "    sentence = re.sub(r\"(\\'re| r )\", \" are \", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would \", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will \", sentence)\n",
    "    sentence = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", sentence)\n",
    "    # Make word separations\n",
    "    sentence = re.sub(r\"(\\+|-|\\*|\\/|\\^|\\.)\", \" $1 \", sentence)\n",
    "    # Remove irrelevant stuff, nonprintable characters and spaces\n",
    "    sentence = re.sub(r\"(\\'s|\\'S|\\'|\\\"|,|[^ -~]+)\", \"\", sentence)\n",
    "    sentence = tc.strip_all(sentence)[0]\n",
    "\n",
    "    if sentence == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    return tc.token_it(tc.lemming(sentence))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:28:41.921644Z",
     "iopub.status.busy": "2021-06-07T17:28:41.920743Z",
     "iopub.status.idle": "2021-06-07T17:33:46.888424Z",
     "shell.execute_reply": "2021-06-07T17:33:46.887586Z",
     "shell.execute_reply.started": "2021-06-07T17:28:41.921605Z"
    },
    "id": "m9ZmfpGWpiuI",
    "outputId": "d2995c9a-92b4-4892-d34b-c77b94b27134"
   },
   "outputs": [],
   "source": [
    "for idx in range(len(S1_train_words)):\n",
    "    S1_train[idx] = data_tokenizer(S1_train_words[idx])\n",
    "\n",
    "for idx in range(len(S2_train_words)):\n",
    "    S2_train[idx] = data_tokenizer(S2_train_words[idx])\n",
    "    \n",
    "for idx in range(len(S1_test_words)): \n",
    "    S1_test[idx] = data_tokenizer(S1_test_words[idx])\n",
    "\n",
    "for idx in range(len(S2_test_words)): \n",
    "    S2_test[idx] = data_tokenizer(S2_test_words[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDcxEmX31y3d"
   },
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 Converting a sentence to a tensor\n",
    "\n",
    "The next step is to convert every sentence to a tensor, or an array of numbers, using the word embeddings loaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:34:56.245768Z",
     "iopub.status.busy": "2021-06-07T17:34:56.245339Z",
     "iopub.status.idle": "2021-06-07T17:35:12.334635Z",
     "shell.execute_reply": "2021-06-07T17:35:12.333772Z",
     "shell.execute_reply.started": "2021-06-07T17:34:56.245739Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Converting sentences to vectors. OOV words or stopwords will be discarded.\n",
    "S1_train_vec = np.empty_like(S1_train)\n",
    "for i in range(len(S1_train)):\n",
    "    S1_train_vec[i] = np.zeros((300,))\n",
    "    for word in S1_train[i]:\n",
    "        if word not in stop_words and word in vec_model.key_to_index:\n",
    "            S1_train_vec[i] += vec_model[word]\n",
    "    S1_train[i] = S1_train_vec[i] / len(S1_train[i])\n",
    "\n",
    "S2_train_vec = np.empty_like(S2_train)\n",
    "for i in range(len(S2_train)):\n",
    "    S2_train_vec[i] = np.zeros((300,))\n",
    "    for word in S2_train[i]:\n",
    "        if word not in stop_words and word in vec_model.key_to_index:\n",
    "            S2_train_vec[i] += vec_model[word]\n",
    "    S2_train[i] = S2_train_vec[i] / len(S2_train[i])\n",
    "\n",
    "\n",
    "S1_test_vec = np.empty_like(S1_test)\n",
    "for i in range(len(S1_test)):\n",
    "    S1_test_vec[i] = np.zeros((300,))\n",
    "    for word in S1_test[i]:\n",
    "        if word not in stop_words and word in vec_model.key_to_index:\n",
    "            S1_test_vec[i] += vec_model[word]\n",
    "    S1_test[i] = S1_test_vec[i] / len(S1_test[i])\n",
    "\n",
    "S2_test_vec = np.empty_like(S2_test)\n",
    "for i in range(len(S2_test)):\n",
    "    S2_test_vec[i] = np.zeros((300,))\n",
    "    for word in S2_test[i]:\n",
    "        if word not in stop_words and word in vec_model.key_to_index:\n",
    "            S2_test_vec[i] += vec_model[word]\n",
    "    S2_test[i] = S2_test_vec[i] / len(S2_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:35:12.336433Z",
     "iopub.status.busy": "2021-06-07T17:35:12.336002Z",
     "iopub.status.idle": "2021-06-07T17:35:12.370785Z",
     "shell.execute_reply": "2021-06-07T17:35:12.369833Z",
     "shell.execute_reply.started": "2021-06-07T17:35:12.336403Z"
    },
    "id": "Dpawm38dpiuU",
    "outputId": "ef1aa65b-c89b-46f9-a9cf-f73748f1ee56"
   },
   "outputs": [],
   "source": [
    "print(\"FIRST SENTENCE IN TRAIN SET:\\n\")\n",
    "print(S1_train_words[0], \"\\n\") \n",
    "print(\"ENCODED VERSION:\")\n",
    "print(S1_train[0],\"\\n\")\n",
    "del(S1_train_words)\n",
    "del(S2_train_words)\n",
    "\n",
    "print(\"FIRST SENTENCE IN TEST SET:\\n\")\n",
    "print(S1_test_words[0], \"\\n\")\n",
    "print(\"ENCODED VERSION:\")\n",
    "print(S1_test[0])\n",
    "del(S1_test_words)\n",
    "del(S2_test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuggGPaQpiuY"
   },
   "source": [
    "Now, the train set must be split into a training/validation set so that it can be used to train and evaluate the Siamese model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:35:12.372420Z",
     "iopub.status.busy": "2021-06-07T17:35:12.372106Z",
     "iopub.status.idle": "2021-06-07T17:35:12.379870Z",
     "shell.execute_reply": "2021-06-07T17:35:12.378979Z",
     "shell.execute_reply.started": "2021-06-07T17:35:12.372390Z"
    },
    "id": "BmhrWPtgpiuY",
    "outputId": "7272fb74-79e6-499a-ce95-d11b9edcd64a"
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "cut_off = int(len(S1_train) * .8)\n",
    "train_S1, train_S2 = S1_train[: cut_off], S2_train[: cut_off]\n",
    "val_S1, val_S2 = S1_train[cut_off :], S2_train[cut_off :]\n",
    "print(\"Number of duplicate sentences: \", len(S1_train))\n",
    "print(\"The length of the training set is:  \", len(train_S1))\n",
    "print(\"The length of the validation set is: \", len(val_S1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFOR19cX2TQs"
   },
   "source": [
    "<a name='1.3'></a>\n",
    "### 1.3 Understanding and building the iterator \n",
    "\n",
    "Given the compational limits, we need to split our data into batches. In this notebook, I built a data generator that takes in $S1$ and $S2$ and returned a batch of size `batch_size` in the following format $([s1_1, s1_2, s1_3, ...]$, $[s2_1, s2_2,s2_3, ...])$. The tuple consists of two arrays and each array has `batch_size` sentences. Again, $s1_i$ and $s2_i$ are duplicates, but they are not duplicates with any other elements in the batch. \n",
    "\n",
    "The command `next(data_generator)` returns the next batch. This iterator returns a pair of arrays of sentences, which will later be used in the model.\n",
    "\n",
    "**The ideas behind:**  \n",
    "- The generator returns shuffled batches of data. To achieve this without modifying the actual sentence lists, a list containing the indexes of the sentences is created. This list can be shuffled and used to get random batches everytime the index is reset.\n",
    "- Append elements of $S1$ and $S2$ to `input1` and `input2` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:35:12.381970Z",
     "iopub.status.busy": "2021-06-07T17:35:12.381673Z",
     "iopub.status.idle": "2021-06-07T17:35:12.393167Z",
     "shell.execute_reply": "2021-06-07T17:35:12.392001Z",
     "shell.execute_reply.started": "2021-06-07T17:35:12.381940Z"
    },
    "id": "ibchgos48MtA"
   },
   "outputs": [],
   "source": [
    "def data_generator(S1, S2, batch_size, shuffle=False):\n",
    "    \"\"\"Generator function that yields batches of data\n",
    "\n",
    "    Args:\n",
    "        S1 (list): List of transformed (to tensor) sentences.\n",
    "        S2 (list): List of transformed (to tensor) sentences.\n",
    "        batch_size (int): Number of elements per batch.\n",
    "        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to False.\n",
    "    Yields:\n",
    "        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)\n",
    "        NOTE: input1: inputs to your model [s1a, s2a, s3a, ...] i.e. (s1a,s1b) are duplicates\n",
    "              input2: targets to your model [s1b, s2b,s3b, ...] i.e. (s1a,s2i) i!=a are not duplicates\n",
    "    \"\"\"\n",
    "\n",
    "    input1 = []\n",
    "    input2 = []\n",
    "    idx = 0\n",
    "    len_s = len(S1)\n",
    "    sentence_indexes = [*range(len_s)]\n",
    "\n",
    "    if shuffle:\n",
    "        rnd.shuffle(sentence_indexes)\n",
    "\n",
    "    while True:\n",
    "        if idx >= len_s:\n",
    "            # If idx is greater than or equal to len_q, reset it\n",
    "            idx = 0\n",
    "            # Shuffle to get random batches if shuffle is set to True\n",
    "            if shuffle:\n",
    "                rnd.shuffle(sentence_indexes)\n",
    "\n",
    "        s1 = S1[sentence_indexes[idx]]\n",
    "        s2 = S2[sentence_indexes[idx]]\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "        input1.append(s1)\n",
    "        input2.append(s2)\n",
    "\n",
    "        if len(input1) == batch_size:\n",
    "            b1 = []\n",
    "            b2 = []\n",
    "            for s1, s2 in zip(input1, input2):\n",
    "                # Append s1\n",
    "                b1.append(s1)\n",
    "                # Append s2\n",
    "                b2.append(s2)\n",
    "\n",
    "            # Use b1 and b2\n",
    "            yield np.array(b1).reshape((batch_size, 1, -1)), np.array(b2).reshape((batch_size, 1, -1))\n",
    "\n",
    "            # reset the batches\n",
    "            input1, input2 = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:35:12.395252Z",
     "iopub.status.busy": "2021-06-07T17:35:12.394777Z",
     "iopub.status.idle": "2021-06-07T17:35:12.414270Z",
     "shell.execute_reply": "2021-06-07T17:35:12.413363Z",
     "shell.execute_reply.started": "2021-06-07T17:35:12.395207Z"
    },
    "id": "ZFZeBPnW8Mlb",
    "outputId": "7a31cd19-55dc-4b97-f288-6c59c6a34b53",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "res1, res2 = next(data_generator(train_S1, train_S2, batch_size))\n",
    "print(\"First sentences  :\\n\", res1, \"\\n Shape: \", res1.shape)\n",
    "print(\"Second sentences :\\n\", res2, \"\\n Shape: \", res2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWJ1L9m2piui"
   },
   "source": [
    "Now we can go ahead and start building the neural network, as we have a data generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmZRBoaMwt0w"
   },
   "source": [
    "<a name='2'></a>\n",
    "# Part 2: Defining the Siamese model\n",
    "\n",
    "<a name='2.1'></a>\n",
    "\n",
    "### 2.1 Understanding and building the Siamese Network \n",
    "\n",
    "A Siamese network is a neural network which uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. The Siamese network model proposed in this notebook looks like this:\n",
    "\n",
    "<img src=\"media/siamese.png\" style=\"width:600px;height:300px;\"/>\n",
    "\n",
    "The sentences' embeddings are passed to an LSTM layer, the output vectors, $v_1$ and $v_2$, are normalized, and finally a triplet loss is used to get the corresponding cosine similarity for each pair of sentences. The triplet loss makes use of a baseline (anchor) input that is compared to a positive (truthy) input and a negative (falsy) input. The distance from the baseline (anchor) input to the positive (truthy) input is minimized, and the distance from the baseline (anchor) input to the negative (falsy) input is maximized. In math equations, the following is maximized:\n",
    "\n",
    "$$\\mathcal{L}(A, P, N)=\\max \\left(\\|\\mathrm{f}(A)-\\mathrm{f}(P)\\|^{2}-\\|\\mathrm{f}(A)-\\mathrm{f}(N)\\|^{2}+\\alpha, 0\\right)$$\n",
    "\n",
    "$A$ is the anchor input, for example $s1_1$, $P$ the duplicate input, for example, $s2_1$, and $N$ the negative input (the non duplicate sentence), for example $s2_2$.<br>\n",
    "$\\alpha$ is a margin - how much the duplicates are pushed from the non duplicates. \n",
    "<br>\n",
    "\n",
    "**The ideas behind:**\n",
    "- Trax library is used in implementing the model.\n",
    "- `tl.Serial`: Combinator that applies layers serially (by function composition) allowing the set up the overall structure of the feedforward.\n",
    "- `tl.LSTM` The LSTM layer.    \n",
    "- `tl.Mean`: Computes the mean across a desired axis. Mean uses one tensor axis to form groups of values and replaces each group with the mean value of that group.\n",
    "- `tl.Fn` Layer with no weights that applies the function f - vector normalization in this case.\n",
    "- `tl.parallel`: It is a combinator layer (like `Serial`) that applies a list of layers in parallel to its inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:35:12.416786Z",
     "iopub.status.busy": "2021-06-07T17:35:12.416486Z",
     "iopub.status.idle": "2021-06-07T17:35:12.424894Z",
     "shell.execute_reply": "2021-06-07T17:35:12.423971Z",
     "shell.execute_reply.started": "2021-06-07T17:35:12.416757Z"
    },
    "id": "hww76f8_wt0x"
   },
   "outputs": [],
   "source": [
    "def Siamese(d_model=300):\n",
    "    \"\"\"Returns a Siamese model.\n",
    "\n",
    "    Args:\n",
    "        d_model (int, optional): Depth of the model. Defaults to 128.\n",
    "        mode (str, optional): \"train\", \"eval\" or \"predict\", predict mode is for fast inference. Defaults to \"train\".\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Parallel: A Siamese model. \n",
    "    \"\"\"\n",
    "\n",
    "    def normalize(x):  # normalizes the vectors to have L2 norm 1\n",
    "        return x / fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n",
    "\n",
    "    s_processor = tl.Serial(                        # Processor will run on S1 and S2.\n",
    "        tl.LSTM(d_model),                           # LSTM layer\n",
    "        tl.Mean(axis=1),                            # Mean over columns\n",
    "        tl.Fn('Normalize', lambda x: normalize(x))  # Apply normalize function\n",
    "    )  # Returns one vector of shape [batch_size, d_model].\n",
    "    \n",
    "    # Run on S1 and S2 in parallel.\n",
    "    model = tl.Parallel(s_processor, s_processor)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "es2gfwZypiul"
   },
   "source": [
    "Setup the Siamese network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:35:12.426380Z",
     "iopub.status.busy": "2021-06-07T17:35:12.426075Z",
     "iopub.status.idle": "2021-06-07T17:35:12.441463Z",
     "shell.execute_reply": "2021-06-07T17:35:12.440667Z",
     "shell.execute_reply.started": "2021-06-07T17:35:12.426352Z"
    },
    "id": "kvQ_jf52-JAn",
    "outputId": "d409460d-2ffb-4ae6-8745-ddcfa1d892ad"
   },
   "outputs": [],
   "source": [
    "# Check the model\n",
    "model = Siamese(d_model=300)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVo1Gvripiuo"
   },
   "source": [
    "<a name='2.2'></a>\n",
    "\n",
    "### 2.2 Implementing Hard  Negative Mining\n",
    "\n",
    "\n",
    "Now it's the time to implement the `TripletLoss`.\n",
    "As explained earlier, loss is composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the *closest negative*. The loss expression is then:\n",
    " \n",
    "\\begin{align}\n",
    " \\mathcal{Loss_1(A,P,N)} &=\\max \\left( -cos(A,P)  + mean_{neg} +\\alpha, 0\\right) \\\\\n",
    " \\mathcal{Loss_2(A,P,N)} &=\\max \\left( -cos(A,P)  + closest_{neg} +\\alpha, 0\\right) \\\\\n",
    "\\mathcal{Loss(A,P,N)} &= mean(Loss_1 + Loss_2) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:35:12.443091Z",
     "iopub.status.busy": "2021-06-07T17:35:12.442581Z",
     "iopub.status.idle": "2021-06-07T17:35:12.452428Z",
     "shell.execute_reply": "2021-06-07T17:35:12.451626Z",
     "shell.execute_reply.started": "2021-06-07T17:35:12.443045Z"
    },
    "id": "oJM8EQiopiuv"
   },
   "outputs": [],
   "source": [
    "def TripletLossFn(v1, v2, margin=0.25):\n",
    "    \"\"\"Custom Loss function.\n",
    "\n",
    "    Args:\n",
    "        v1 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to S1.\n",
    "        v2 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to S2.\n",
    "        margin (float, optional): Desired margin. Defaults to 0.25.\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Triplet Loss.\n",
    "    \"\"\"\n",
    "\n",
    "    scores = fastnp.dot(v1, v2.T)       # pairwise cosine sim\n",
    "    batch_size = len(scores)\n",
    "\n",
    "    positive = fastnp.diagonal(scores)  # the positive ones (duplicates)\n",
    "    negative_without_positive = scores - 2.0 * fastnp.eye(batch_size)\n",
    "\n",
    "    closest_negative = fastnp.max(negative_without_positive, axis=1)\n",
    "    negative_zero_on_duplicate = (1.0 - fastnp.eye(batch_size)) * scores\n",
    "    mean_negative = fastnp.sum(negative_zero_on_duplicate, axis=1) / (batch_size - 1)\n",
    "\n",
    "    triplet_loss1 = fastnp.maximum(0.0, margin - positive + closest_negative)\n",
    "    triplet_loss2 = fastnp.maximum(0.0, margin - positive + mean_negative)\n",
    "    triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n",
    "    \n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:35:12.454023Z",
     "iopub.status.busy": "2021-06-07T17:35:12.453564Z",
     "iopub.status.idle": "2021-06-07T17:35:12.717565Z",
     "shell.execute_reply": "2021-06-07T17:35:12.716836Z",
     "shell.execute_reply.started": "2021-06-07T17:35:12.453979Z"
    }
   },
   "outputs": [],
   "source": [
    "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
    "v2 = np.array([[0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
    "TripletLossFn(v2,v1)\n",
    "print(\"Triplet Loss:\", TripletLossFn(v2,v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "Triplet Loss: 1.0\n",
    "```   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:35:12.719277Z",
     "iopub.status.busy": "2021-06-07T17:35:12.718787Z",
     "iopub.status.idle": "2021-06-07T17:35:12.723868Z",
     "shell.execute_reply": "2021-06-07T17:35:12.722909Z",
     "shell.execute_reply.started": "2021-06-07T17:35:12.719231Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def TripletLoss(margin=1):\n",
    "    # Trax layer creation\n",
    "    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n",
    "    return tl.Fn(\"TripletLoss\", triplet_loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsvjaCQ6wt02"
   },
   "source": [
    "<a name='3'></a>\n",
    "\n",
    "# Part 3: Training\n",
    "\n",
    "The next step is model training - defining the cost function and the optimizer, feeding in the built model. But first I will define the data generators used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:35:12.725548Z",
     "iopub.status.busy": "2021-06-07T17:35:12.725272Z",
     "iopub.status.idle": "2021-06-07T17:35:12.741888Z",
     "shell.execute_reply": "2021-06-07T17:35:12.740519Z",
     "shell.execute_reply.started": "2021-06-07T17:35:12.725521Z"
    },
    "id": "iPk7gh-nzCBg",
    "outputId": "a2e8525d-f89a-4d9d-c0d6-bd7406f0246a"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_generator = data_generator(train_S1, train_S2, batch_size)\n",
    "val_generator = data_generator(val_S1, val_S2, batch_size)\n",
    "print(\"train_S1.shape \", train_S1.shape)\n",
    "print(\"val_S1.shape   \", val_S1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgFMfH5awt07"
   },
   "source": [
    "Now, I will define the training step. Each training iteration is defined as an `epoch`, each epoch being an iteration over all the data, using the training iterator.\n",
    "\n",
    "**The ideas behind:**\n",
    "- Two tasks are needed: `TrainTask` and `EvalTask`.\n",
    "- The training runs in a trax loop `trax.supervised.training.Loop`.\n",
    "- Pass the other parameters to a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:35:12.743736Z",
     "iopub.status.busy": "2021-06-07T17:35:12.743419Z",
     "iopub.status.idle": "2021-06-07T17:35:12.754061Z",
     "shell.execute_reply": "2021-06-07T17:35:12.752847Z",
     "shell.execute_reply.started": "2021-06-07T17:35:12.743707Z"
    },
    "id": "_kbtfz4T_m7x"
   },
   "outputs": [],
   "source": [
    "def train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir=\"trax_model/\"):\n",
    "    \"\"\"Training the Siamese Model\n",
    "\n",
    "    Args:\n",
    "        Siamese (function): Function that returns the Siamese model.\n",
    "        TripletLoss (function): Function that defines the TripletLoss loss function.\n",
    "        lr_schedule (function): Trax multifactor schedule function.\n",
    "        train_generator (generator, optional): Training generator. Defaults to train_generator.\n",
    "        val_generator (generator, optional): Validation generator. Defaults to val_generator.\n",
    "        output_dir (str, optional): Path to save model to. Defaults to \"trax_model/\".\n",
    "\n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir = os.path.expanduser(output_dir)\n",
    "\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=train_generator,\n",
    "        loss_layer=TripletLoss(),\n",
    "        optimizer=trax.optimizers.Adam(0.01),\n",
    "        lr_schedule=lr_schedule\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=val_generator,\n",
    "        metrics=[TripletLoss()]\n",
    "    )\n",
    "\n",
    "    training_loop = training.Loop(Siamese(),\n",
    "                                  train_task,\n",
    "                                  eval_tasks=[eval_task],\n",
    "                                  output_dir=output_dir,\n",
    "                                  random_seed=34)\n",
    "\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:35:12.755853Z",
     "iopub.status.busy": "2021-06-07T17:35:12.755557Z",
     "iopub.status.idle": "2021-06-07T17:36:34.046883Z",
     "shell.execute_reply": "2021-06-07T17:36:34.045729Z",
     "shell.execute_reply.started": "2021-06-07T17:35:12.755825Z"
    },
    "id": "-3KXjmBo_6Xa",
    "outputId": "9d57f731-1534-4218-e744-783359d5cd19"
   },
   "outputs": [],
   "source": [
    "train_steps = 1500\n",
    "lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\n",
    "training_loop = train_model(Siamese, TripletLoss, lr_schedule)\n",
    "training_loop.run(train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abKPe7d4wt1C"
   },
   "source": [
    "<a name='4'></a>\n",
    "\n",
    "# Part 4:  Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDi4MBiKpivF"
   },
   "source": [
    "To determine the accuracy of the model, the test set that was configured earlier is used. While the training used only positive examples, the test data, S1_test, S2_test and y_test, is setup as pairs of sentences, some of which are duplicates some are not. \n",
    "This routine runs all the test sentences pairs through the model, computes the cosine simlarity of each pair, thresholds it and compares the result to  y_test - the correct response from the data set. The results are accumulated to produce an accuracy.\n",
    "\n",
    "**The ideas behind:**  \n",
    " - The model loops through the incoming data in batch_size chunks.\n",
    " - The output vectors are computed and their cosine similarity is thresholded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:48:05.214835Z",
     "iopub.status.busy": "2021-06-07T17:48:05.214468Z",
     "iopub.status.idle": "2021-06-07T17:48:05.223298Z",
     "shell.execute_reply": "2021-06-07T17:48:05.222101Z",
     "shell.execute_reply.started": "2021-06-07T17:48:05.214795Z"
    },
    "id": "K-h6ZH507fUm"
   },
   "outputs": [],
   "source": [
    "def classify(test_S1, test_S2, y, threshold, model, data_generator=data_generator, batch_size=64):\n",
    "    \"\"\"Function to test the model. Calculates some metrics, such as precision, accuracy, recall and F1 score.\n",
    "\n",
    "    Args:\n",
    "        test_S1 (numpy.ndarray): Array of S1 sentences.\n",
    "        test_S2 (numpy.ndarray): Array of S2 sentences.\n",
    "        y (numpy.ndarray): Array of actual target.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (trax.layers.combinators.Parallel): The Siamese model.\n",
    "        data_generator (function): Data generator function. Defaults to data_generator.\n",
    "        batch_size (int, optional): Size of the batches. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        (float, float, float, float): Accuracy, precision, recall and F1 score of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "\n",
    "    for i in range(0, len(test_S1), batch_size):\n",
    "        to_process = len(test_S1) - i\n",
    "\n",
    "        if to_process < batch_size:\n",
    "            batch_size = to_process\n",
    "\n",
    "        s1, s2 = next(data_generator(test_S1[i : i + batch_size], test_S2[i : i + batch_size], batch_size, shuffle=False))\n",
    "        y_test = y[i : i + batch_size]\n",
    "\n",
    "        v1, v2 = model((s1, s2))\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            d = np.dot(v1[j], v2[j].T)\n",
    "            res = d > threshold\n",
    "\n",
    "            if res == 1:\n",
    "                if y_test[j] == res:\n",
    "                    true_pos += 1\n",
    "                else:\n",
    "                    false_pos += 1\n",
    "            else:\n",
    "                if y_test[j] == res:\n",
    "                    true_neg += 1\n",
    "                else:\n",
    "                    false_neg += 1\n",
    "\n",
    "    accuracy = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return (accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(S1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:48:09.501375Z",
     "iopub.status.busy": "2021-06-07T17:48:09.500951Z",
     "iopub.status.idle": "2021-06-07T17:50:53.267851Z",
     "shell.execute_reply": "2021-06-07T17:50:53.266534Z",
     "shell.execute_reply.started": "2021-06-07T17:48:09.501337Z"
    },
    "id": "yeQjHxkfpivH",
    "outputId": "103b8449-896f-403d-f011-583df70afdae"
   },
   "outputs": [],
   "source": [
    "# Loading in the saved model\n",
    "model = Siamese()\n",
    "model.init_from_file(\"trax_model/model.pkl.gz\")\n",
    "# Evaluating it\n",
    "accuracy, precision, recall, f1_score = classify(S1_test, S2_test, y_test, 0.7, model, batch_size=512) \n",
    "print(\"Accuracy\", accuracy)\n",
    "print(\"Precision\", precision)\n",
    "print(\"Recall\", recall)\n",
    "print(\"F1 score\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21h3Y0FNpivK"
   },
   "source": [
    "<a name='5'></a>\n",
    "\n",
    "# Part 5: Making predictions\n",
    "\n",
    "In this section the model will be put to work. It will be wrapped in a function called `predict` which takes two sentences as input and returns $1$ or $0$, depending on whether the pair is a duplicate or not.   \n",
    "\n",
    "But first, we need to embed the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kg0wQ8qhpivL"
   },
   "outputs": [],
   "source": [
    "def predict(sentence1, sentence2, threshold, model, data_generator=data_generator, verbose=False):\n",
    "    \"\"\"Function for predicting if two sentences are duplicates.\n",
    "\n",
    "    Args:\n",
    "        sentence1 (str): First sentence.\n",
    "        sentence2 (str): Second sentence.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (trax.layers.combinators.Parallel): The Siamese model.\n",
    "        data_generator (function): Data generator function. Defaults to data_generator.\n",
    "        verbose (bool, optional): If the results should be printed out. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the sentences are duplicates, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    s1 = data_tokenizer(sentence1)     # tokenize\n",
    "    S1 = np.zeros((300,))\n",
    "    for word in s1:\n",
    "        if word not in stop_words and word in vec_model.key_to_index:\n",
    "            S1 += vec_model[word]\n",
    "    S1 = S1 / len(s1)\n",
    "    \n",
    "    s2 = data_tokenizer(sentence2)     # tokenize\n",
    "    S2 = np.zeros((300,))\n",
    "    for word in s2:\n",
    "        if word not in stop_words and word in vec_model.key_to_index:\n",
    "            S1 += vec_model[word]\n",
    "    S2 = S2 / len(s2)\n",
    "\n",
    "    S1, S2 = next(data_generator([S1], [S2], 1))\n",
    "\n",
    "    v1, v2 = model((S1, S2))\n",
    "    d = np.dot(v1[0], v2[0].T)\n",
    "    res = d > threshold\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(\"S1  = \", S1, \"\\nS2  = \", S2)\n",
    "        print(\"d   = \", d)\n",
    "        print(\"res = \", res)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the model's ability to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Raojyhw3z7HE",
    "outputId": "b0907aaf-63c0-448d-99b0-012359381a97"
   },
   "outputs": [],
   "source": [
    "sentence1 = \"I love running in the park.\"\n",
    "sentence2 = \"I like running in park?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(sentence1 , sentence2, 0.7, model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAfV3l5Zwt1L"
   },
   "source": [
    "The Siamese network is capable of catching complicated structures. Concretely, it can identify sentence duplicates although the sentences do not have many words in common."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
